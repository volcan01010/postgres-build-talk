<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
		<meta name="description" content="Presentation slides for PostgreSQL Build, December 2021">
		<meta name="author" content="Dr John A Stevenson">

		<title>How many Postgres is too many?</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/bgs.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
<section>
	<section>
		<h4>How many Postgres is too many?</h4>
		<p><span class="small">An infrastructure-as-code approach to PostgreSQL-backed IoT sensor data sharing</span></p>
		<p>
		<span class="small"><a href="https://twitter.com/volcan01010">Dr John A Stevenson</a> / <a href="https://twitter.com/volcan01010">@volcan01010</a></span>
		</p>
		<a href="https://www.bgs.ac.uk"><img src="images/BGS-Logo-Pos-RGB.svg" width=400 style="background:white; border:15px solid white; box-shadow:none;"></a>
		<aside data-markdown class="notes">
		The aim of this talk is to describe the architecture of BGS Sensor Data Service, the tools that we used to create it and some (hopefully helpful) hints that we learned in the process.

		As the main title suggests, this talk contains some code examples.
		Don't worry if you don't understand them; I'll explain what each does and the most important thing is to get a sense of how these systems are configured.
		</aside>
	</section>
	<section>
		<h4>Talk outline</h4>
		<ul>
		<li>Background</li>
		<li>Single server configuration</li>
		<li>Multi-environments and replication</li>
		<li>Questions for discussion</li>
		</ul>
		<aside data-markdown class="notes">
			Our approach is different to the cloud-native, Kubernetes-based setups described in the other talks.
			Hopefully there will time for feedback and discussion at the end.

		</aside>
	</section>
</section>

<section>
	<section>
		<h4>About BGS</h4>
		<blockquote>"Our mission is to provide impartial and independent geoscience advice and data."</blockquote>
		<ul>
		<li>Work with academics, governments, industry and the public</li>
		<li>Digital division has 10 years of making digital data available e.g. via <a href="http://onegeology.org/">OneGeology</a> and <a href="https://www.bgs.ac.uk/geological-data/opengeoscience/">OpenGeoscience Portal</a>
		and contributions to standards for data sharing e.g. <a href="https://www.bgs.ac.uk/news/geosciml-data-standard-becomes-official/">GeoSciML</a></li>
		</ul>
	<aside data-markdown class="notes">
		We have 650 staff, 70 in Digital are professional software developers and database experts.
		This sensor data service described here is part of our recent approach of increasingly sharing data via APIs.
	</aside>
	</section>

	<section>
		<h4>BGS IT Infrastructure</h4>
		<ul>
		<li>Self-hosted virtual machines running on premises</li>
		<li>Long-standing corporate Oracle database</li>
		<li>Increasing use of PostgreSQL for microservices and collaborative projects</li>
		</ul>
	<aside data-markdown class="notes">
		Self-hosting our VMs means that we do not pay per minute; it is not expensive to run multiple servers with low loads.
		Microservices often include their own PostgreSQL databases as part of the software stack.
		If we use PostgreSQL, we can share workflows with partners in academia or countries such as Kyrgyzstan and Uganda.
	</aside>
	</section>

	<section>
		<img class="stretch" src="images/glasgow_main_mine_working.png">
		<a href="https://ukgeos.ac.uk/observatories/glasgow">UK Geoenergy Observatories</a>
	<aside data-markdown class="notes">
		For the UKGEOS project, we have placed instruments down boreholes to create natural laboratories.
		UKGEOS Glasgow is investigating the use of water in abandoned coal mines to heat homes in a similar way to a ground source heat pump.
	</aside>
	</section>

	<section>
		<img class="stretch" src="images/sensor_data_graph.png">
		<a href="https://sensors.bgs.ac.uk">sensors.bgs.ac.uk</a>
	<aside data-markdown class="notes">
		The BGS Sensor Data Service allows the data to be accessed via an api.
		It is powered by FROST-Server, which is an open-source project serving the data via the Open Geospatial Consortium's SensorThings API standard.

		+ [API root](https://sensors.bgs.ac.uk/FROST-Server/v1.1)
		+ [Observed properties include water temperature](https://sensors.bgs.ac.uk/FROST-Server/v1.1/ObservedProperties)
		+ [Datastreams for water temperature in different locations](https://sensors.bgs.ac.uk/FROST-Server/v1.1/ObservedProperties(3)/Datastreams)
		+ [Observations for borehole GGA07](https://sensors.bgs.ac.uk/FROST-Server/v1.1/Datastreams(20)/Observations)

		The water temperature is around 11°C.
	</aside>
	</section>
</section>

<section>
	<section data-background-size="contain" data-background="images/001_just_postgres.png">
	<aside data-markdown class="notes">
		This is a PostgreSQL conference, so we should centre the talk around PostgreSQL.

		Here is a single PostgreSQL database.

		I like it because it is easy to create many instances, which gives flexiblity and scope for test environments.
		You will see this as the talk goes on.
	</aside>
	</section>

	<section>
		<h4>Running PostgreSQL in Docker</h4>
		<ul>
		<li>Images as small as 150 Mb, spin up in seconds</li>
		<li>Same on the server as on developer laptop</li>
		<li>Cluster settings are applied (e.g. pg_hba.conf) with setup scripts</li>
		<li>Configuration stored in /var/lib/postgres/data</li>
		</ul>
	<aside data-markdown class="notes">
		Docker is an easy way to install PostgreSQL.
		You can define scripts that are run when the container starts for the first time.
		These can be used to configure things such as the `pg_hba.conf`, which controls who can connect.
		All those settings are stored in `/var/lib/postgres/data`.
		Mounting `/var/lib/postgres/data` as a volume provides persistent data between changes to the container.
	</aside>
	</section>

	<section data-background-size="contain" data-background="images/001_just_postgres.png">
	<aside data-markdown class="notes">
		Of course the application is more than just a database...
	</aside>
	</section>
	<section data-transition="fade-in" data-background-size="contain" data-background="images/002_single_vm.png">
	<aside data-markdown class="notes">
		The API services comprise a PostgreSQL database and FROST-Server running on a virtual machine.
		We use docker-compose to start the services together.
		FROST-Server connects to the database and creates the tables that it needs using Liquibase.
	</aside>
	</section>

	<section>
		<pre><code data-trim data-line-numbers="2,10,11,13,21,25,26">
services:
  db:
    image: .../frost-db:${FROST_DB_TAG}
    ...
    environment:
      POSTGRES_DB: frost_db
      POSTGRES_USER: ${FROST_DB_USER}
      POSTGRES_PASSWORD: ${FROST_DB_PASSWORD}
    ...
    volumes:
      - postgis_data:/var/lib/postgresql/data

  frost:
    image: .../frost-server:${FROST_API_TAG}
    ...
    environment:
      ...
      - persistence_db_url=jdbc:postgresql://db:5432/frost_db
      - persistence_db_username=${FROST_DB_USER}
      - persistence_db_password=${FROST_DB_PASSWORD}
    command: ["/wait_for_postgis.sh", "catalina.sh", "run"]
    depends_on:
      - db

volumes:
  postgis_data:
		</code></pre>
		docker-compose.yml
	<aside data-markdown class="notes">
		The docker-compose file describes the database (db) and FROST services.

		For the database, note the environment variables used to configure the superuser account and the volume to ensure that data persist.

		For the FROST Server, note the same credentials and the custom command to run when the container starts.
		This command ensures that the database is ready before FROST tries to create the tables.
	</aside>
	</section>

	<section>
		<pre><code data-trim>
#!/bin/sh
cmd="$@"
  
until PGPASSWORD=${persistence_db_password} psql \
  -h db -p 5432 -d frost_db -U ${persistence_db_username} \
  -c '\q'; do
  >&2 echo "Postgres is unavailable - sleeping"
  sleep 2
done

sleep 2  # Allow another 2 seconds for it to self-configure
>&2 echo "Postgres is up - executing command"
exec $cmd
		</code></pre>
		wait_for_postgres.sh
	<aside data-markdown class="notes">
		The `wait_for_postgres.sh` script captures the arguments passed to it in the `$cmd` variable.
		It then loops over a call to `psql -c '\q'`, which simply tries to connect to the database then quit.
		After the command succeeds, it calls `exec $cmd` to run the command in the remaining arguments.
	</aside>
	</section>

	<section>
		<pre><code data-trim data-line-numbers="1,6,13,20">
- name: Copy host_files to deployment directory
  copy:
    src: host_files/
    dest: "{{ deploy_dir }}"

- name: Write .env file with environment variables for table creation
  copy:
    dest: "{{ deploy_dir }}/.env"
    content: "{{ env_file_content_for_setup }}"

...

- name: Reset database
  docker_compose:
    project_src: "{{ deploy_dir }}"
    state: absent
    remove_volumes: yes
  when: reset_database

- name: Call docker-compose up
  docker_compose:
    project_src: "{{ deploy_dir }}"
    state: present
    recreate: always
		</code></pre>
		deploy.yml (Ansible playbook)
	<aside data-markdown class="notes">
		We use Ansible to deploy the application and database to the server.
		Ansible is a server-configuration tool that can SSH onto a target machine and run scripts there.
		The scripts are declarative, so they define the end state rather than the steps to get there.
		They are also idempotent, which means they can be run repeatedly and always give the same result.

		The scripts copy across the `docker-compose.yml` file and create a `.env` file containing the environment variables.
		They then call `docker-compose up` to start the service.

		We have an extra step to reset the database, which can be triggered via an environment variable.
		This will drop the data volume and give us a fresh database when the service restarts.
	</aside>
	</section>

	<section data-background-size="contain" data-background="images/002_single_vm.png">
	<aside data-markdown class="notes">
		Once our service is running, we need to populate it with some data.
	</aside>
	</section>
	<section data-transition="fade-in" data-background-size="contain" data-background="images/003_single_vm_and_etl.png">
	<aside data-markdown class="notes">
		The sensor data are stored in the corporate Oracle database, where they are highly normalised and integrated with other internal data structures.
		We wrote a custom Python tool called `etlhelper` to transfer the data to the FROST servers.
		`etlhelper` solves two problems:

		+ It installs and configures the Oracle Instant Client on Linux, which can be a bit fiddly
		+ It provides a simple way to run a SQL query against a database and get the results back using Python

		`etlhelper` runs a query against Oracle and then posts the results to the FROST server via the API.
		Using the API allows FROST to take care of all the internal foreign table relationships.
		The asynchronous `aiohttp` Python library made inserting data much quicker.
	</aside>
	</section>

	<section>
		<blockquote>etlhelper is a Python library to simplify data transfer between databases</blockquote>
		<img class="stretch" src="images/etlhelper_image.png">
		<a href="https://github.com/BritishGeologicalSurvey/etlhelper">https://github.com/BritishGeologicalSurvey/etlhelper</a>
	<aside data-markdown class="notes">
		`etlhelper` is aimed at anyone who just wants to run a SQL query against a database and insert the results into another one.

		In the example above, `etlhelper` copies the data from the "src" table on "conn1" to the "exported" table on "conn2".
		Behind the scenes it handles things like cursors and transactions and transferring the data in chunks so that it doesn't take up too much space in memory.

		We open-sourced `etlhelper` two years ago, so you can look it up on GitHub and install it using `pip`.
		Please "star" the repo if it is useful to you.
	</aside>
	</section>
</section>

<section>
	<section data-background-size="contain" data-background="images/003_single_vm_and_etl.png">
	<aside data-markdown class="notes">
		Once we have our pipeline working, it's time to create some environments.
	</aside>
	</section>
	<section data-transition="fade-in" data-background-size="contain" data-background="images/004_add_environments.png">
	<aside data-markdown class="notes">
		Ansible will apply configuration to servers listed in the `hosts.yml` file.
		To create a new environment is simply a case of starting a server and adding it to the list with appropriate settings for the environment variables.
		We use GitLab CI to run the deployment scripts.

		The Development environment allows us to experiment with ETL and server configuration without modifying the Internal server.

		We now have 2 PostgreSQLs.
	</aside>
	</section>

	<section>
		<img class="stretch" src="images/pglog_table_screenshot.png">
		Logs exported as CSV can be queried as a foreign table
	<aside data-markdown class="notes">
		The DBAs don't have SSH to the servers to access the Docker containers and check their log files.
		This can make debugging tricky.

		Luckily PostgreSQL can make the logs available as a table that can be inspected via PgAdmin or other tools.
		This is done by configuring the database to write logs to a CSV file.
		The file is then mounted as a table using the `file_fdw` foreign data wrapper.
		
		The image shows the logs from the server startup process.
	</aside>
	</section>

	<section>
		<pre><code data-trim data-line-numbers="10,11">
services:
  db:
    image: .../frost-db:${FROST_DB_TAG}
    ...
    environment:
      POSTGRES_DB: frost_db
      POSTGRES_USER: ${FROST_DB_USER}
      POSTGRES_PASSWORD: ${FROST_DB_PASSWORD}
    ...
    command: ["postgres", "-c", "logging_collector=on", "-c", "log_destination=csvlog",
              "-c", "log_filename=postgresql"]
    volumes:
      - postgis_data:/var/lib/postgresql/data

volumes:
  postgis_data:
		</code></pre>
		postgresql.conf settings can be overridden at startup
	<aside data-markdown class="notes">
		The logging settings are defined in the `postgresql.conf` file.
		Unfortunately this file is created in the server initialisation step and would be fiddly to change in a running Docker container.

		Fortunately, PostgreSQL allows you to override some configuration parameters via command-line arguements.
		We can set up the logging by using a custom `command` in the docker-compose file.
	</aside>
	</section>

	<section data-background-size="contain" data-background="images/004_add_environments.png">
	<aside data-markdown class="notes">
		The ETL process was set up as a scheduled task to migrate new data each day.
		To ensure that this metadata was created correctly, we used test-driven development (TDD) to write the ETL scripts.
		This required an extra development database that could be wiped clean repeatedly when the test suite was run.
		(We could also have done this with a database running on our laptops but not everyone had access to Docker.)
	</aside>
	</section>
	<section data-transition="fade-in" data-background-size="contain" data-background="images/005_integration_test.png">
	<aside data-markdown class="notes">
		It was easy to create an integration test server and add it to the Ansible hosts list.

		We now have 3 PostgreSQLs.
	</aside>
	</section>

	<section>
		Jobs table update code written by test-driven development
		<pre><code data-trim>
./db/migrations/
├── common
│   ├── V001__Create_jobs_table.sql
│   ├── V002__Create_job_stats_table.sql
│   ├── V003__Create_roles.sql
│   ...
│   └── V012__Log_to_table.sql
├── hwlapidev001
│   ├── flyway.conf
│   └── sql
│       ├── V005__Configure_replication.sql
│       └── V007__Configure_replication_delete_provider.sql
├── hwlapidev002
│   ├── flyway.conf
│   └── sql
│       ├── V005__Configure_replication.sql
│       └── V007__Configure_replication_delete_provider.sql
...
		</pre></code>
		Flyway is used to apply SQL scripts to configure databases
	<aside data-markdown class="notes">
		Flyway is an open source tool providing "version control for databases".
		It can run a series of SQL scripts against a remote server sequentially.
		We run it in our configuration pipeine from GitLab CI.

		The clever part about Flyway is that it records metadata about which have been run so they can be skipped the next time.
		It also stores a checksum for each file so it can see if the contents have changed.

		The files above contain the SQL scripts for creating the job loggins tables and more.
		Those in the `common` directory are run against all servers.
		There are also folders with specific configuration for individual servers.
		These are used to set up `pglogical` replication, as we will see later.
	</aside>
	</section>
	
	<section data-background-size="contain" data-background="images/005_integration_test.png">
	<aside data-markdown class="notes">
		The next step is to add a Public environment and redundancy for each server.
	</aside>
	</section>
	<section data-transition="fade-in" data-background-size="contain" data-background="images/006_add_replication.png">
	<aside data-markdown class="notes">
		There are three big changes to notice here.

		Firstly, we have added a Public environment.
		Not all the data are for public consumption.
		Some belong to other projects and others need to undergo checking before release.
		The Public environment allows us to publish a subset of the data.

		Secondly, there are now pairs of servers in each environment.
		The external network access to the servers balances the load between them, giving us redundancy.
		We are not Google and do not expect millions of hits, however it is important to keep the sensor data service running when a server needs to reboot to apply a kernel update.

		Thirdly, the new sensors are being populated via `pglogical` from the internal server and not via `etlhelper`.
		Populating the database via the API can be slow and, because the API calls happen asynchronously, they can result in data objects arriving in different orders and receiving differet internal ids.
		(We also store the Oracle ID on each object, so we could track it back, but it is still better if they match).

		`pglogical` is an open source Logical Replication extension for PostgreSQL.
		It provides an easy to way synchronise data tables and thus populate the data without using the API.
		It is easy to configure and data transfer is almost instantaneous.
		As a bonus using the API on the subscriber services means it can be made read-only, which increases security.

		A row filter on the `frost_public` replication set ensures that we only publish the public data.

		We now have 6 PostgreSQLs.
	</aside>
	</section>

	<section>
		<pre><code data-trim>
SELECT pglogical.create_node(
    node_name := 'stage001',
    dsn := 'host=xxxx port=5432 dbname=frost_db user=xxxx password=${pglogical_upd_password}');

SELECT pglogical.create_replication_set('frost_all');

SELECT pglogical.replication_set_add_table(
	set_name := 'frost_public',
	relation := '"DATASTREAMS"',
	row_filter := ' "PROPERTIES" ->> ''publish_yn'' = ''Y'' ');
		</pre></code>
	pglogical replication is configured via SQL commands
	<aside data-markdown class="notes">
		The commands above demonstrate SQL commands for configuring `pglogical`
		Each server has to be created as a "node".
		Replication sets are defined on the publisher nodes.

		Subscriber nodes are configured by passing connection details to publishing server and the name of the replica set.

		We use Flyway to run these configuration scripts.
	</aside>
	</section>

	<section data-background-size="contain" data-background="images/006_add_replication.png">
	<aside data-markdown class="notes">
		It could be risky to make changes to replication sets on the live environments.
		It would be useful to be able to test these in development.
	</aside>
	</section>
	<section data-transition="fade-in" data-background-size="contain" data-background="images/007_replication_test.png">
	<aside data-markdown class="notes">
		Here we have added a third development server.
		We have also added a second PostgreSQL database in each server.
		This allows us to use Dev2 and Dev3 to test the replication scripts.

		In this case we didn't need to add a whole extra server, we just add a new database to the existing PostgreSQL clusters.
		The new databases are created by copying the main database as soon as the tables have been created by FROST.
		At this point they are empty and available for futher configuration with Flyway.

		We now have 10 PostgreSQLs.  I think that's enough.
	</aside>
	</section>

	<section>
		<pre><code data-trim>
# Create database, stopping here if it already exists
if psql -U frost_db_superuser -d frost_db \
    -c "CREATE DATABASE frost_db2 WITH OWNER frost_db_superuser;"
then
  echo "Created database frost_db2"
else
  echo "Database frost_db2 already exists"
  exit 0
fi

# Copy data
echo "Copying contents and configuration from frost_db"
pg_dump -U frost_db_superuser -d frost_db > /tmp/frost_db.sql
psql -U frost_db_superuser -d frost_db2 -f /tmp/frost_db.sql

# Tidy up
rm /tmp/frost_db.sql
		</pre></code>
	An idempotent script to clone a running database
	<aside data-markdown class="notes">
		This is the script used to clone the database during the pipeline.

		It is important that the script is idempotent as it will be run each the time CI pipeline runs, yet it only needs to create the database once.
		The first part of the script exits without error if the database already exists.

		The second part of the script uses `pg_dump` to backup the main database, the `psql` to apply the configuration.
	</aside>
	</section>

	<section>
		<pre><code data-trim>
hosts: dev
  tasks:
    - name: Create frost_db2 as copy of frost_db
      command: >
          docker-compose exec db /bin/bash /create_frost_db2.sh
      args:
        chdir: "{{ deploy_dir }}"
		</pre></code>
	Storing script in container keeps CI logic clean
	<aside data-markdown class="notes">
		One final tip: we found it was cleaner to write the script and to bake it into the database container than to try to execute the commands directly from the CI pipeline code.

		This code snippet shows the Ansible task that runs the script against the servers in the `dev` group.
	</aside>
	</section>

</section>

<section>
	<section data-background-size="contain" data-background="images/007_replication_test_desaturated.png">
		<h4>Take-home messages</h4>
		<ol>
			<li>We use Docker and docker-compose to install PostgreSQL</li>
			<li>Use <i>etlhelper</i> to to simplify data transfer with Python</li>
			<li>Ansible and GitLab CI are used to deploy to servers</li>
			<li>PostgreSQL can serve logs as a table using <i>file_fdw</i></li>
			<li>Flyway runs database configuration via tracked SQL scripts</li>
			<li>Test-driven development is easier with a dedicated test database</li>
			<li>pglogical replication is easy to configure and <b>fast</b></li>
		</ol>
	<aside data-markdown class="notes">
		This slide wasn't in the presentation as recorded, but was added later to reiterate the main points.
	</aside>
	</section>
	<section data-background-size="contain" data-background="images/007_replication_test_desaturated.png">
		<h4>Questions for discussion</h4>
		<ul>
			<li>Is 10 PostgreSQLs too many?</a>
			<li>Is it common to use pglogical as part of ETL process?</li>
			<li>What are the pros/cons of using a database pool?</li>
			<li>What if we had to pay by the minute for servers?</li>
		</ul>
	<aside data-markdown class="notes">
		+ I don't think that 10 is too many.  When they are defined in code they are easy and quick to create and having all the different environments gives us a lot of flexibility in testing.
		+ I was really impressed by how fast and simple `pglogical` was.  I could imagine a situation where you could write the results of a complex join into a table and have it replicated out somewhere else to be used.
		+ This setup is simple e.g. one database and one API, paired on each server.  You could also use `pgbouncer` to pool the two databases, then have as many API servers as you want pointed at that.  Unless traffic is very high, I'm not sure that it would justify the extra complexity.
		+ It is cheap for us to have servers that run all the time but under minimal load.  If you have to reduce the server count, you could run multiple PostgreSQL clusters on a server on different ports, but you still need enough for redundancy after reboots.  Are there any other ways to consolidate them?
	</aside>
	</section>
	<section>
		<h4>Thank you</h4>
	</section>
</section>

	</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,
				width: 1152,
				height: 840,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
